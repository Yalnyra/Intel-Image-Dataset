{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 269359,
          "sourceType": "datasetVersion",
          "datasetId": 111880
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yalnyra/Intel-Image-Dataset/blob/main/Vinokur_Model_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- У класі \"street\" і \"glacier\" багато чорно-білих фото, спробуємо grayscale зображень. Зображення вдень і вночі, в різні часи доби і освітлення. Різна розмірність фічей, як от куполів на фото, тому спробувати Resize."
      ],
      "metadata": {
        "id": "qdXFjklVx6AB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Зображення чіткі, шумів не знайдено,\n",
        "\n",
        "Припускаємо із візуалізованих даних, що всі належать до одного з 5 класів і відсутні wildcard класи.\n",
        "\n",
        "\n",
        "TODO: Спробуємо знайти \"щільність\" кластерів даних об'єктів за допомогою UMAP, t-SNE чи PCA, іншої кластеризації для візуалізації відстаней, і позначити кольором до яких класів кожен датапоінт належить.\n",
        "\n",
        "Dot-product зображень, t-test на корреляцію.\n",
        "Корреляцію між зображеннями, scatter plot по PCA"
      ],
      "metadata": {
        "id": "G-VZovelx6AD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для обробки поки не виконуємо нормалізацію, треба невелику\n",
        "- аугментація 20% зображень із одним з трьох варіантів\n",
        " - випадковим розширенням і поворотом для scale/affine invariance\n",
        " - Перетворенням на чорно-біле зображення\n",
        " 0. Для великого зображення 150*150*3 збільшимо к-сть згорток з 4 до 6, з розмірами фіч мап\n",
        " 0. Макс 4*4 пулінг\n",
        " 1. Батч нормалізація\n",
        "\n",
        "Що буде зроблено на даному етапі\n",
        "<!-- - Зробимо взважування по частоті -->\n",
        "-\n",
        "<!-- - Спробуємо спочатку постійний learning rate, потім ще додамо з Lr scheduler. -->\n",
        "- У Adam додамо параметр регуляризації\n"
      ],
      "metadata": {
        "id": "uWe3Rf0Wx6AE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Фінальний проєкт\n",
        "## Класифікація зображень за допомогою PyTorch\n",
        "\n",
        "**Мета роботи: Навчитись тренувати базову нейронну мережу для класифікації зображень.**\n",
        "\n",
        "**Вимоги:**\n",
        "1. Препроцессинг, train/pred, тест повністю на розмічених зображеннях.\n",
        "\n",
        "\n",
        " Аугментація: blur/grayscale/affine transform/noise\n",
        "\n",
        "2. Архітектура:\n",
        "\n",
        " - Пару CNN шарів, 2-3 Linear\n",
        "\n",
        " - Max/median pooling, batch norm\n",
        "\n",
        "3. Lr_rate finetuning,\n"
      ],
      "metadata": {
        "id": "oZYE2cPLx6AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Для colab потрібно опціональні частини torch\n",
        "# %pip install torchvision"
      ],
      "metadata": {
        "id": "68qvqe78x6AG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-16T14:34:43.409638Z",
          "iopub.execute_input": "2025-03-16T14:34:43.409977Z",
          "iopub.status.idle": "2025-03-16T14:34:43.414202Z",
          "shell.execute_reply.started": "2025-03-16T14:34:43.409947Z",
          "shell.execute_reply": "2025-03-16T14:34:43.412878Z"
        }
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# Image load\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from tempfile import TemporaryDirectory\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "# import torchvision\n",
        "from torchvision.transforms import v2 as transforms\n",
        "from torchvision import datasets, models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.utils import make_grid\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, ConfusionMatrixDisplay\n",
        "# import wandb\n"
      ],
      "metadata": {
        "id": "5k1KQiAPx6AH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-16T14:34:45.554169Z",
          "iopub.execute_input": "2025-03-16T14:34:45.554496Z",
          "iopub.status.idle": "2025-03-16T14:34:45.560059Z",
          "shell.execute_reply.started": "2025-03-16T14:34:45.554471Z",
          "shell.execute_reply": "2025-03-16T14:34:45.559098Z"
        }
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "jBi2zJ5sx6AI",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-16T14:34:49.873521Z",
          "iopub.execute_input": "2025-03-16T14:34:49.873844Z",
          "iopub.status.idle": "2025-03-16T14:34:49.90435Z",
          "shell.execute_reply.started": "2025-03-16T14:34:49.873819Z",
          "shell.execute_reply": "2025-03-16T14:34:49.903304Z"
        }
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeBQoTmIx6AI",
        "outputId": "0a345afb-6301-4d87-a7ab-8c88d93313e6",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "Якщо працюєте із kaggle - встановіть"
      ],
      "metadata": {
        "id": "FDSBUKMzZEVy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TLmLI3KPam03"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
        "URL = \"drive/MyDrive/Intel-Image-Classification\"\n",
        "# URL = \"/kaggle/input/intel-image-classification\"\n",
        "folder = {\"kaggle\": 'seg_pred/seg_pred',\n",
        "          'train': 'seg_train/seg_train',\n",
        "          'test': 'seg_test/seg_test'\n",
        "          }\n",
        "BATCH_SIZE = 64\n",
        "_NUM_WORKERS = os.cpu_count() // 2\n",
        "_INPUT_SIZE = 150\n",
        "SEED = 1234"
      ],
      "metadata": {
        "id": "ZxAWbz9Mx6AJ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def to_device(X, y):\n",
        "    return X.to(device), y.to(device, dtype=torch.int64)"
      ],
      "metadata": {
        "id": "97AtqisSx6AK",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(seed=SEED)"
      ],
      "metadata": {
        "id": "Vw0XaIcIx6AK",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "# @title loading image from Intel Image folder\n",
        "# @markdown # Запишемо всі лейбли і шляхи до файлів, -1 відповідає за непомічені дані\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, train=True, transform=None, generator=rng):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.img_label_pairs = []\n",
        "        self.classes = {}\n",
        "        self.subdir = 'seg_train/seg_train' if train else 'seg_test/seg_test'\n",
        "\n",
        "        # Collect all image paths and their corresponding labels\n",
        "        subdir_path = os.path.join(root_dir, self.subdir)\n",
        "        for class_name in os.listdir(subdir_path):\n",
        "            class_path = os.path.join(subdir_path, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                if class_name not in self.classes:\n",
        "                    self.classes[class_name] = len(self.classes)\n",
        "                for filename in os.listdir(class_path):\n",
        "                    if filename.endswith('.jpg'):\n",
        "                        self.img_label_pairs.append(\n",
        "                            (os.path.join(class_path, filename),self.classes[class_name]))\n",
        "\n",
        "        # # Collect images from seg_pred (unlabeled data)\n",
        "        # unlabeled = os.path.join(root_dir, 'seg_pred/seg_pred')\n",
        "        # for filename in os.listdir(unlabeled):\n",
        "        #     if filename.endswith('.jpg'):\n",
        "        #         self.img_label_pairs.append(\n",
        "        #         (os.path.join(unlabeled, filename),-1)\n",
        "        #         )\n",
        "\n",
        "        rng.shuffle(self.img_label_pairs)\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_tensor(image):\n",
        "        return transforms.Compose([\n",
        "            # transforms.ToImage(),\n",
        "            # transforms.ToDtype(torch.float32, scale=True)\n",
        "            transforms.ToTensor()\n",
        "            ])(image)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_label_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_path, label = self.img_label_pairs[idx]\n",
        "        image = ImageDataset._to_tensor(Image.open(img_path).convert('RGB'))\n",
        "        label = torch.tensor(label, dtype=torch.int)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Example usage:\n",
        "# Define any transformations you want to apply to the images\n",
        "transform = transforms.Compose([\n",
        "    # transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create the dataset\n",
        "dataset = ImageDataset(root_dir=URL, transform=transform)\n",
        "\n",
        "# Access an image tensor and its label\n",
        "image_tensor, label = dataset[0]\n",
        "print(image_tensor.shape, label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwBSzjS6x6AL",
        "outputId": "4e3da9ee-5849-4d63-c30c-06b8e0b6d1e8",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 150, 150]) tensor(3, dtype=torch.int32)\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "Імплементація зверху дуже повільна"
      ],
      "metadata": {
        "id": "8DA4gX91rHjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LA0pUglsG0fV",
        "outputId": "efda1ec8-6761-4d58-817f-a311f59a1e30"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/MyDrive/Intel-Image-Classification'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Args:\n",
        "    root_dir (string): Directory with all the images.\n",
        "    transform (callable, optional): Optional transform to be applied on a sample.\n",
        "\"\"\"\n",
        "\n",
        "train_dir = os.path.join(URL, folder['train'])\n",
        "test_dir  = os.path.join(URL, folder['test'])"
      ],
      "metadata": {
        "id": "K7LEC_E-pYMP",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "# Повний тренувальний датасет\n",
        "data_train = datasets.ImageFolder(train_dir, transform=transforms.ToDtype(torch.float32, scale=True))\n",
        "\n",
        "# Тестовий датасет\n",
        "data_test = datasets.ImageFolder(test_dir, transforms.ToDtype(torch.float32, scale=True))"
      ],
      "metadata": {
        "id": "ub_PXjptpUcQ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDOdco9AJWrn",
        "outputId": "79778c2e-0e57-4dd1-c6eb-d66d92cf50d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_train = ImageDataset(URL, train=True)\n",
        "# data_test = ImageDataset(URL, train=False)"
      ],
      "metadata": {
        "id": "dP7j-J4px6AP",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.classes"
      ],
      "metadata": {
        "id": "zlc371oYpSXH",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a2a5b8-f33f-4350-a779-35a5286587db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "classes = data_train.classes"
      ],
      "metadata": {
        "id": "1icbjDEywUtk",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "# classes = [k for k in data_train.c.keys()]"
      ],
      "metadata": {
        "id": "BPv2J24bpCMg",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, data_valid = random_split(data_train, lengths=[0.8, 0.2])"
      ],
      "metadata": {
        "id": "ZBCTm2F0x6AP",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "class WrappedDataLoader:\n",
        "    def __init__(self, loader, func):\n",
        "        self.loader = loader\n",
        "        self.func = func\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.loader)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch in iter(self.loader):\n",
        "            yield self.func(*batch)"
      ],
      "metadata": {
        "id": "Ymc7Lorgx6AP",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "loader_train = WrappedDataLoader(DataLoader(data_train,\n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True,\n",
        "                                            num_workers=_NUM_WORKERS), to_device)\n",
        "loader_eval = WrappedDataLoader(DataLoader(data_valid,\n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            shuffle=False,\n",
        "                                            num_workers=_NUM_WORKERS), to_device)\n",
        "loader_test = WrappedDataLoader(DataLoader(data_test,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           shuffle=False,\n",
        "                                            num_workers=_NUM_WORKERS), to_device)"
      ],
      "metadata": {
        "id": "ljVJ-kw8x6AQ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "affine_transforms = transforms.Compose([\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.15, 0.15), scale=(0.85, 1.15), interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "    transforms.RandomResizedCrop(size=(150, 150), scale=(0.15, 1), ratio=(1,1), antialias=True),\n",
        "    transforms.RandomHorizontalFlip(p=0.),\n",
        "])\n",
        "\n",
        "augmentations = transforms.Compose([\n",
        "    transforms.GaussianNoise(sigma=0.12),\n",
        "    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.0)),\n",
        "])"
      ],
      "metadata": {
        "id": "qzpqqSLRx6AQ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "repeats = 50\n",
        "features_size = BATCH_SIZE * repeats"
      ],
      "metadata": {
        "id": "Dz0ATOvWx6AQ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((150,150), interpolation=transforms.InterpolationMode.BILINEAR, max_size=None, antialias=True),\n",
        "    transforms.CenterCrop((150,150)),\n",
        "    ])"
      ],
      "metadata": {
        "id": "BIBxEsMvxUMJ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "# features_mean = torch.zeros(1)\n",
        "# features_squared_mean = torch.zeros(1)\n",
        "# # Only use a few batches\n",
        "# for item, label in data_train:\n",
        "#     item = test_transforms(item)\n",
        "#     features_mean += torch.mean(item)\n",
        "#     features_squared_mean += torch.mean(item ** 2)\n",
        "#     print(repeats)\n",
        "#     repeats -= 1\n",
        "#     if repeats <= 0:\n",
        "#       break\n",
        "\n",
        "# features_mean /= features_size\n",
        "# features_squared_mean /= features_size\n",
        "\n",
        "# features_std = torch.sqrt(features_squared_mean - features_mean ** 2)"
      ],
      "metadata": {
        "id": "GtlAbn3mx6AQ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [
        "color_transforms = transforms.Compose([\n",
        "    # transforms.Normalize(features_mean, features_std),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.1, saturation=0.01),\n",
        "])"
      ],
      "metadata": {
        "id": "KvU-Ja04x6AP",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    # transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((150,150), interpolation=transforms.InterpolationMode.BILINEAR, max_size=None, antialias=True),\n",
        "    transforms.CenterCrop((150,150)),\n",
        "    transforms.RandomApply([\n",
        "    transforms.RandomChoice([\n",
        "        affine_transforms,\n",
        "        augmentations,\n",
        "        color_transforms,\n",
        "    ])\n",
        "  ], p=0.2),\n",
        "  transforms.RandomGrayscale(p=0.2),\n",
        "])\n",
        "\n",
        "valid_transforms = transforms.Compose([\n",
        "    # transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((150,150), interpolation=transforms.InterpolationMode.BILINEAR, max_size=None, antialias=True),\n",
        "    transforms.CenterCrop((150,150)),\n",
        "    transforms.RandomApply([\n",
        "    transforms.RandomChoice([\n",
        "        affine_transforms,\n",
        "    ])\n",
        "  ], p=0.2),\n",
        "])"
      ],
      "metadata": {
        "id": "P4vKbo463CVM",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(model, loss_func, X, y, optimizer=None):\n",
        "    loss_ = loss_func(model(X), y)\n",
        "    if optimizer is not None:\n",
        "      loss_.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    return loss_.item(), len(X)"
      ],
      "metadata": {
        "id": "Dqf725t6x6AR",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, loss_func, X, y):\n",
        "    output = model(X)\n",
        "    loss_ = loss_func(output, y)\n",
        "    pred = torch.argmax(output, dim=1)\n",
        "    correct = pred == y.view(*pred.shape)\n",
        "\n",
        "    return loss_.item(), torch.sum(correct).item(), len(X), pred, y.view(*pred.shape)"
      ],
      "metadata": {
        "id": "8IjSjzVQx6AR",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(epochs, model, loss_func, optimizer, train_loader, valid_loader, lr_scheduler=None, patience=10):\n",
        "    graphic_losses = []\n",
        "\n",
        "    wait = 0\n",
        "    valid_loss_min = np.Inf\n",
        "    step = 0\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        step = epoch\n",
        "        model.train()\n",
        "\n",
        "        losses = []\n",
        "        for X, y  in train_loader:\n",
        "          # print(X.shape)\n",
        "          losses.append(loss(model, loss_func, X, y, optimizer))\n",
        "\n",
        "        losses, nums = zip(*losses)\n",
        "        train_loss = sum(np.multiply(losses, nums)) / sum(nums)\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            losses = []\n",
        "            for X, y in valid_loader:\n",
        "              losses.append(validate(model, loss_func, X, y))\n",
        "\n",
        "            losses, corrects, nums, predicted, target = zip(*losses)\n",
        "            target = torch.cat([*target], dim=0).cpu()\n",
        "            predicted = torch.cat([*predicted],dim=0).cpu()\n",
        "            valid_loss = sum(np.multiply(losses, nums)) / sum(nums)\n",
        "            valid_accuracy = sum(corrects) / sum(nums) * 100\n",
        "            # Changed from total class per-point  to average weighted\n",
        "            valid_precision = precision_score(target, predicted, average='weighted')\n",
        "            valid_recall = recall_score(target, predicted,  average='weighted')\n",
        "            valid_f1 = f1_score(target, predicted,  average='weighted')\n",
        "\n",
        "\n",
        "            print(f\"\\nepoch: {epoch+1:3}, loss: {train_loss:.5f}, valid loss: {valid_loss:.5f}, valid accuracy: {valid_accuracy:.3f}, valid f1: {valid_f1:.3f}%\")\n",
        "\n",
        "            graphic_losses.append((train_loss, valid_loss, valid_accuracy, valid_precision, valid_recall, valid_f1))\n",
        "            # Save model if validation loss has decreased\n",
        "            if valid_loss <= valid_loss_min:\n",
        "                print(f\"Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...\")\n",
        "                torch.save(model.state_dict(), 'model.pt')\n",
        "                valid_loss_min = valid_loss\n",
        "                wait = 0\n",
        "            # Early stopping\n",
        "            else:\n",
        "                wait += 1\n",
        "                if wait >= patience:\n",
        "                    print(f\"Terminated Training for Early Stopping at Epoch {epoch+1}\")\n",
        "                    return graphic_losses, step\n",
        "\n",
        "    return graphic_losses, step"
      ],
      "metadata": {
        "id": "os2YRvL7x6AR",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 66
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Тестування моделі\n",
        "\n",
        "- На додачу до простого Accuracy, потрібно знайти\n",
        "Precision\n",
        "$TP \\div (TP+FP)$\n",
        "\n",
        "Recall\n",
        "$TN \\div (TN+FN)$\n",
        "\n",
        "F1 $\\frac{1}{\\frac{1}{Precision}+\\frac{1}{Recall}} $\n",
        "\n",
        "Матрицю зкуйовдженості"
      ],
      "metadata": {
        "id": "UVKAREr3x6AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loss_func, loader):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        validated_batches = []\n",
        "\n",
        "        for X, y in loader:\n",
        "          validated_batches.append(validate(model, loss_func, X, y))\n",
        "\n",
        "        losses, corrects, nums, predicted, target = zip(*validated_batches)\n",
        "        target = torch.cat([*target], dim=0).cpu()\n",
        "        predicted = torch.cat([*predicted],dim=0).cpu()\n",
        "        cm = ConfusionMatrixDisplay.from_predictions(target, predicted)\n",
        "        plt.show()\n",
        "        test_loss = sum(np.multiply(losses, nums)) / sum(nums)\n",
        "        test_accuracy = sum(corrects) / sum(nums) * 100\n",
        "        test_precision = precision_score(target, predicted,  average='micro')\n",
        "        test_recall = recall_score(target, predicted,  average='micro')\n",
        "        test_f1 = f1_score(target, predicted,  average='micro')\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Test loss: {test_loss:.5f}\\t\",\n",
        "          f\"Test accuracy: {test_accuracy:.3f}%\",\n",
        "          f\"Test precision: {test_precision:.3f}\",\n",
        "          f\"Test recall: {test_recall:.3f}\"\n",
        "          f\"Test f1: {test_f1:.3f}\")\n",
        "    return test_loss, test_accuracy, test_precision, test_recall, test_f1, cm"
      ],
      "metadata": {
        "id": "LKodaUqKx6AR",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import make_interp_spline"
      ],
      "metadata": {
        "id": "FuFOOaO-niQ8",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": [
        "# # Створюємо координати X для детальної кривої навчання\n",
        "# batches_per_epoch = len(train_loader)\n",
        "# total_batches = batches_per_epoch * epochs\n",
        "# batch_x = np.linspace(0, epochs, total_batches)\n",
        "\n",
        "# # Створюємо координати X для середніх втрат за епохи\n",
        "# epoch_x = np.array(range(1, epochs + 1)) - 0.5 # Зміщуємо на -0.5 для кращого візуального узгодження\n",
        "\n",
        "# # Створюємо плавну криву для валідаційних втрат\n",
        "# if len(val_losses) > 2:  # Потрібно щонайменше 3 точки для сплайна\n",
        "#     x_smooth = np.linspace(epoch_x.min(), epoch_x.max(), 100)\n",
        "#     spl = make_interp_spline(\n",
        "#         epoch_x, val_losses, k=min(2, len(val_losses) - 1)\n",
        "#     )  # k визначає порядок сплайна\n",
        "#     val_losses_smooth = spl(x_smooth)\n",
        "# else:\n",
        "#     x_smooth = epoch_x\n",
        "#     val_losses_smooth = val_losses\n",
        "\n",
        "# # Палітра кольорів для більш професійного вигляду\n",
        "# train_batch_color = \"#8ECAE6\"  # світло-блакитний\n",
        "# train_epoch_color = \"#0077B6\"  # насичений синій\n",
        "# val_color = \"#FB8500\"  # помаранчевий\n",
        "\n",
        "# # Візуалізація втрат\n",
        "# plt.figure(figsize=(12, 6))\n",
        "\n",
        "# # Графік детальних втрат тренування\n",
        "# plt.plot(\n",
        "#     batch_x,\n",
        "#     batch_train_losses,\n",
        "#     train_batch_color,\n",
        "#     alpha=0.3,\n",
        "#     label=\"Втрати по батчах\",\n",
        "#     linewidth=1.5,\n",
        "# )\n",
        "\n",
        "# # Графік середніх втрат на тренуванні\n",
        "# plt.plot(\n",
        "#     epoch_x,\n",
        "#     train_losses,\n",
        "#     \"o-\",\n",
        "#     color=train_epoch_color,\n",
        "#     linewidth=2.5,\n",
        "#     markersize=8,\n",
        "#     label=\"Середні втрати на тренуванні\",\n",
        "# )\n",
        "\n",
        "# # Графік валідаційних втрат (згладжені лінія + точки на одній кривій)\n",
        "# plt.plot(\n",
        "#     x_smooth,\n",
        "#     val_losses_smooth,\n",
        "#     color=val_color,\n",
        "#     linewidth=2.5,\n",
        "#     label=\"Втрати на валідації\",\n",
        "# )\n",
        "# # Додаємо оригінальні точки на тій самій кривій\n",
        "# plt.scatter(\n",
        "#     epoch_x, val_losses, color=val_color, s=65, zorder=5, edgecolor=\"white\", linewidth=1\n",
        "# )\n",
        "\n",
        "# plt.xlabel(\"Епохи\", fontsize=14)\n",
        "# plt.ylabel(\"Втрати\", fontsize=14)\n",
        "# plt.title(\"Криві навчання (детальна візуалізація)\", fontsize=16)\n",
        "# plt.grid(alpha=0.3)\n",
        "# plt.legend(fontsize=12)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Psds4UKLnnPZ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": [
        "def training_plots(losses_arr):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot([x[2] for x in losses_arr])\n",
        "    plt.ylabel('Accuracy in %')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.xticks([x + 1 for x in range(len(losses_arr)) if x % 2 == 1])\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot([x[3] for x in losses_arr])\n",
        "    plt.ylabel('Precision')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.xticks([x + 1 for x in range(len(losses_arr)) if x % 2 == 1])\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot([x[4] for x in losses_arr])\n",
        "    plt.ylabel('Recall')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.xticks([x + 1 for x in range(len(losses_arr)) if x % 2 == 1])\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot([x[5] for x in losses_arr])\n",
        "    plt.ylabel('F1')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.xticks([x + 1 for x in range(len(losses_arr)) if x % 2 == 1])\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot([x[0] for x in losses_arr], label='train loss')\n",
        "    plt.plot([x[1] for x in losses_arr], label='validation loss')\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.ylabel('Losses')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.xticks([x + 1 for x in range(len(losses_arr)) if x % 2 == 1])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-qBBdqpbx6AS",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "# Функція для денормалізації зображення\n",
        "def denormalize_image(tensor, mean=0.5, std=0.5):\n",
        "    # tensor має форму (channels, height, width); повертає зображення у форматі HxWxC\n",
        "    img = tensor.cpu().numpy().transpose((1, 2, 0))\n",
        "    img = std * img + mean\n",
        "    return np.clip(img, 0, 1)\n",
        "\n",
        "\n",
        "# Функція для відображення зображень з прогнозами (оновлено)\n",
        "def show_predictions(model, dataloader, class_names, device, num_images=8):\n",
        "    \"\"\"\n",
        "    Відображає приклади зображень з прогнозами моделі.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size(0)):\n",
        "                if images_so_far >= num_images:\n",
        "                    return\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(2, num_images // 2, images_so_far)\n",
        "                ax.axis(\"off\")\n",
        "                status = \"✓\" if preds[j] == labels[j] else \"✗\"\n",
        "                ax.set_title(\n",
        "                    f\"{status} Прогноз: {class_names[preds[j]]}\\nРеальний клас: {class_names[labels[j]]}\",\n",
        "                    color=\"green\" if preds[j] == labels[j] else \"red\",\n",
        "                    fontsize=10,\n",
        "                )\n",
        "                # Використовуємо утиліту для денормалізації\n",
        "                img = denormalize_image(inputs[j])\n",
        "                plt.imshow(img)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Функція для відображення лише помилкових прогнозів (оновлено)\n",
        "def show_misclassifications(model, dataloader, class_names, device, max_per_class=5):\n",
        "    \"\"\"\n",
        "    Відображає приклади помилково класифікованих зображень для кожного класу.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    misclassified = {i: [] for i in range(len(class_names))}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size(0)):\n",
        "                true_label = labels[j].item()\n",
        "                pred_label = preds[j].item()\n",
        "                if (\n",
        "                    true_label != pred_label\n",
        "                    and len(misclassified[true_label]) < max_per_class\n",
        "                ):\n",
        "                    img = denormalize_image(inputs[j])\n",
        "                    misclassified[true_label].append((img, pred_label))\n",
        "\n",
        "    for class_idx, examples in misclassified.items():\n",
        "        if not examples:\n",
        "            continue\n",
        "        plt.figure(figsize=(15, 3))\n",
        "        plt.suptitle(\n",
        "            f\"Помилково класифіковані зображення класу '{class_names[class_idx]}'\",\n",
        "            fontsize=14,\n",
        "        )\n",
        "        for i, (img, pred_label) in enumerate(examples):\n",
        "            plt.subplot(1, len(examples), i + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(\n",
        "                f\"Реальний: {class_names[class_idx]}\\nПрогноз: {class_names[pred_label]}\",\n",
        "                color=\"red\",\n",
        "            )\n",
        "            plt.axis(\"off\")\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "zDp2IxGpoVyj",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 65
    },
    {
      "cell_type": "code",
      "source": [
        "# # Показ прикладів прогнозів на валідаційному наборі\n",
        "# show_predictions(model, valid_loader, classes, device)\n",
        "\n",
        "# # Показ прикладів ПОМИЛКОВИХ прогнозів для кожного класу\n",
        "# print(\"\\nПриклади помилкових класифікацій CNN моделі:\")\n",
        "# show_misclassifications(model, valid_loader, classes, device)"
      ],
      "metadata": {
        "id": "rlaAd4RAoGrx",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = lambda name: f\"out/models/Intel/{name}.pt\""
      ],
      "metadata": {
        "id": "C4dw1s4Ex6AT",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(model_path('')):\n",
        "      os.makedirs(model_path(''))"
      ],
      "metadata": {
        "id": "JjkWYam9x6AT",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms\n",
        "\n",
        "valid_transforms\n",
        "\n",
        "# test_transforms"
      ],
      "metadata": {
        "id": "HbGMOKNRzVzt",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72232b65-0203-4c65-aeb1-ad65154c03ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "      ToTensor()\n",
              "      Resize(size=[150, 150], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
              "      CenterCrop(size=(150, 150))\n",
              "      RandomApply(\n",
              "        RandomChoice(\n",
              "      transforms=[Compose(\n",
              "            RandomAffine(degrees=[-15.0, 15.0], translate=(0.15, 0.15), scale=(0.85, 1.15), interpolation=InterpolationMode.BILINEAR, fill=0)\n",
              "            RandomResizedCrop(size=(150, 150), scale=(0.15, 1), ratio=(1, 1), interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
              "            RandomHorizontalFlip(p=0.0)\n",
              "      )], p=[1.0]\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yYpTATOsJTU4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, n_epochs, batch_size, train_transforms, lr_scheduler=None, saving_model_path=None, wb = False):\n",
        "\n",
        "    # data_train = ImageDataset(URL, True, train_transforms)\n",
        "\n",
        "    # dataset_eval = ImageDataset(URL, valid_transforms)\n",
        "\n",
        "    # data_test = ImageDataset(URL, test_transforms)\n",
        "\n",
        "    data_train = datasets.ImageFolder(train_dir, train_transforms)\n",
        "    data_test = datasets.ImageFolder(test_dir, test_transforms)\n",
        "\n",
        "    data_train, data_valid = random_split(data_train, lengths=[0.8, 0.2])\n",
        "\n",
        "    print(len)\n",
        "\n",
        "    loader_train = WrappedDataLoader(DataLoader(data_train, batch_size=batch_size, shuffle=True), to_device)\n",
        "    loader_eval = WrappedDataLoader(DataLoader(data_valid, batch_size=batch_size, shuffle=False), to_device)\n",
        "    loader_test = WrappedDataLoader(DataLoader(data_test, batch_size=batch_size, shuffle=False), to_device)\n",
        "\n",
        "    print('\\nFitting nn model')\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_losses, final_step = fit(n_epochs, model, criterion, optimizer, loader_train, loader_eval, lr_scheduler)\n",
        "    print(f'length array:', len(train_losses))\n",
        "    print(f'Fit time: {time.time() - start_time} s')\n",
        "    if wb:\n",
        "        wandb.log({\"train/duration\": time.time() - start_time})\n",
        "    check_point = torch.load('model.pt', map_location=device)\n",
        "    model.load_state_dict(check_point)\n",
        "\n",
        "    eval_losses = evaluate(model, criterion, loader_test)\n",
        "\n",
        "    if saving_model_path is not None:\n",
        "        print('Saving model')\n",
        "        # create 'dynamic' dir, if it does not exist\n",
        "        if not os.path.exists(model_path('')):\n",
        "          os.makedirs(model_path(''))\n",
        "        torch.save(model.state_dict(), model_path(saving_model_path))\n",
        "        if wb:\n",
        "            wandb.log_model(path=model_path(saving_model_path),name=saving_model_path)\n",
        "            for i in range(final_step):\n",
        "                train_loss, v_loss, v_acc, v_prec, v_recall, v_f1 = train_losses[i]\n",
        "                wb_dict = {\n",
        "                    \"train/loss\": train_loss,\n",
        "                    \"valid/loss\": v_loss,\n",
        "                    \"valid/accuracy\": v_acc,\n",
        "                    \"valid/precision\": v_prec,\n",
        "                    \"valid/recall\": v_recall,\n",
        "                    \"valid/f1\": v_f1,\n",
        "                           }\n",
        "                wandb.log(wb_dict, step=i)\n",
        "            test_loss, t_acc, t_prec, t_recall, t_f1, fig = eval_losses\n",
        "            wandb.log({\"test/loss\": test_loss,\n",
        "                    \"test/accuracy\": t_acc,\n",
        "                    \"test/precision\": t_prec,\n",
        "                    \"test/recall\": t_recall,\n",
        "                    \"test/f1\": t_f1}, step=final_step+1)\n",
        "            wandb.log({\"fig\":wandb.Image(fig.confusion_matrix)})\n",
        "\n",
        "    training_plots(train_losses)"
      ],
      "metadata": {
        "id": "Td3jLWopx6AT",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 67
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 32 * 3 -> 16 -> 16\n",
        "        self.bn11 = nn.BatchNorm2d(3)\n",
        "        self.conv11 = nn.Conv2d(3, 75, 3, padding=\"same\") #3/16\n",
        "        self.bn12 = nn.BatchNorm2d(75) # 16\n",
        "        self.conv12 = nn.Conv2d(75, 75, 3, padding=\"same\") #16/16\n",
        "        self.bn21 = nn.BatchNorm2d(75) # 16\n",
        "        self.conv21 = nn.Conv2d(75, 450, 3, padding=\"same\") #16/32\n",
        "        self.bn22 = nn.BatchNorm2d(450) # 32\n",
        "        self.conv22 = nn.Conv2d(450, 450, 3, padding=\"same\") #32/32 -> 150 * 37 * 37\n",
        "        # self.bn31 = nn.BatchNorm2d(75) # 16\n",
        "        self.conv31 = nn.Conv2d(450, 1350, 3, padding=\"same\") #16/32 -> 450*9*9\n",
        "        # self.bn32 = nn.BatchNorm2d(150) # 32\n",
        "        self.conv32 = nn.Conv2d(1350, 1350, 3, padding=\"same\") #32/32 -> 1350*2*2\n",
        "        self.fc1 = nn.Linear(5400, 256) # 32 * 64\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, len(classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv11(self.bn11(x)))\n",
        "        x=  F.relu(self.conv12(self.bn12(x)))\n",
        "        x = F.max_pool2d(x, 4)\n",
        "        # print(\"\\n\", x.shape)\n",
        "        x = F.relu(self.conv21(self.bn21(x)))\n",
        "        x = F.relu(self.conv22(self.bn22(x)))\n",
        "        x = F.max_pool2d(x, 4)\n",
        "        # print(\"\\n\", x.shape)\n",
        "        x = F.relu(self.conv31(x))\n",
        "        x = F.relu(self.conv32(x))\n",
        "        x = F.max_pool2d(x, 4)\n",
        "        # print(\"\\n\", x.shape)\n",
        "        x = x.view(-1, 5400)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # x = F.dropout(F.relu(self.fc2(x)), 0.2)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cN3nP7PZx6AT",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 71
    },
    {
      "cell_type": "code",
      "source": [
        "# class_weights = torch.tensor(calculate_class_weights(y_train), dtype=torch.float, device=device)\n",
        "n_epochs = 10\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "ApM7VHARx6AU",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 40
    },
    {
      "cell_type": "code",
      "source": [
        "model = BasicModel().to(device)\n",
        "# criterion = nn.CrossEntropyLoss(class_weights)\n",
        "# criterion = nn.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
        "lr_sch = lr_scheduler.StepLR(optimizer, 30, 0.3)"
      ],
      "metadata": {
        "id": "xSWAsRqBx6AU",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 59
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "8OVb5CDuSZvP",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 42
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "oTSQk9ULx6AV",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "00a661c8-7987-44b9-e178-e3a98c516064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpjedi\u001b[0m (\u001b[33mparadoxv15\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "execution_count": 43
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тренуємо на 75->450->1350 фіч мапах без\n",
        "трансформацій grayscale, нормалізації чи збільшення розміру"
      ],
      "metadata": {
        "id": "jkKDz6Dvb0Nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train(model, criterion, optimizer, n_epochs, BATCH_SIZE, train_transforms, lr_scheduler=lr_sch, saving_model_path='max_model', wb=False)"
      ],
      "metadata": {
        "id": "XlNLoTOYx6AV",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 44
    },
    {
      "cell_type": "code",
      "source": [
        "# model = BetterModel().to(device)\n",
        "# with wandb.init(\n",
        "#         settings=wandb.Settings(start_method=\"thread\"),\n",
        "#         project=\"Kaggle-Intel-Image\",\n",
        "#         name='dropout_tuned',\n",
        "#         group='CNN',\n",
        "#         config={\"epochs\":n_epochs, \"batch\": BATCH_SIZE, \"transforms\": train_transforms},\n",
        "#         sync_tensorboard=False,\n",
        "#         resume=\"allow\",\n",
        "#     ):\n",
        "#         train(model, criterion, optimizer, n_epochs, BATCH_SIZE, train_transforms, lr_scheduler=lr_sch, saving_model_path='max_model', wb=True)"
      ],
      "metadata": {
        "id": "bZG22WH7x6AW",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 45
    },
    {
      "cell_type": "code",
      "source": [
        "classes"
      ],
      "metadata": {
        "id": "jaizKtiNDusB",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f96522-5e24-48ba-e083-42fceb2266ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "execution_count": 46
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Висновки по тренуванню\n",
        "Поки що оверфітиться після 10 епох, 86% непоганий результат\n",
        "\n",
        "З lr 1e-3 впадає у локальний мінімум, з точністю ~67%\n",
        "\n",
        "У матриці плутанини найбільше помилок зустрічається в 3 конкретних класах з 10:\n",
        "\n",
        "2: bird  3: cat 4: deer, меншою мірою 5: dog, тобто фото звірів найменше відрізняються між собою на 32*32 зображеннях.\n",
        "\n",
        "1. ColorJitter і афінні трансформ, lr=0.003 без learning rate decay, з dropout\n",
        "\n",
        "2. Colorjitter, з lr_scheduler, початковий 0.003\n",
        " За 31 епоху дійшло до 74%, далі пішло в плато\n",
        "\n",
        "3. Випадковий поворот (advanced_transforms), lr=0.003\n",
        "\n",
        "- Поступово збігається до 50/50, субоптимальний, не виправдовує робастність\n",
        "\n",
        "4. train_transforms Без colorjitter, lr=0.001 дійшло до 70%, далі пішло в плато.\n",
        "\n",
        "5. Color_jitter, з saturation gain убавленою з 0.2 до 0.05, lr=0.001\n",
        "Після 50 епох дійшло до 77.5% на тестовій вибірці і не збиралось зупинятись\n"
      ],
      "metadata": {
        "id": "3glpaSTkx6AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title З меншим feature map модель, як вплине на продуктивність?\n",
        "\n",
        "class FeatMapModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 32 * 3 -> 16 -> 16\n",
        "        self.bn11 = nn.BatchNorm2d(3)\n",
        "        self.conv11 = nn.Conv2d(3, 16, 3, padding=\"same\") #3/16\n",
        "        self.bn12 = nn.BatchNorm2d(16) # 16\n",
        "        self.conv12 = nn.Conv2d(16, 16, 3, padding=\"same\") #16/16\n",
        "        self.bn21 = nn.BatchNorm2d(16) # 16\n",
        "        self.conv21 = nn.Conv2d(16, 32, 3, padding=\"same\") #16/32\n",
        "        self.bn22 = nn.BatchNorm2d(32) # 32\n",
        "        self.conv22 = nn.Conv2d(32, 32, 3, padding=\"same\") #32/32 -> 150 * 37 * 37\n",
        "        # self.bn31 = nn.BatchNorm2d(75) # 16\n",
        "        self.conv31 = nn.Conv2d(32, 64, 3, padding=\"same\") #16/32 -> 450*9*9\n",
        "        # self.bn32 = nn.BatchNorm2d(150) # 32\n",
        "        self.conv32 = nn.Conv2d(64, 64, 3, padding=\"same\") #32/32 -> 1350*2*2\n",
        "        # self.fc1 = nn.Linear(256, 512) # 8192\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, len(classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv11(self.bn11(x)))\n",
        "        x=  F.relu(self.conv12(self.bn12(x)))\n",
        "        x = F.max_pool2d(x, 4)\n",
        "        # print(\"\\n\", x.shape)\n",
        "        x = F.relu(self.conv21(self.bn21(x)))\n",
        "        x = F.relu(self.conv22(self.bn22(x)))\n",
        "        x = F.max_pool2d(x, 4)\n",
        "        # print(\"\\n\", x.shape)\n",
        "        x = F.relu(self.conv31(x))\n",
        "        x = F.relu(self.conv32(x))\n",
        "        x = F.max_pool2d(x, 4)\n",
        "        # print(\"\\n\", x.shape)\n",
        "        # print(x.shape)\n",
        "        x = x.view(-1, 256)\n",
        "        # print(x.shape)\n",
        "        # x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # x = F.dropout(F.relu(self.fc2(x)), 0.2)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qmcm174eEbbQ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 47
    },
    {
      "cell_type": "code",
      "source": [
        "# feat_model = FeatMapModel().to(device)\n",
        "# with wandb.init(\n",
        "#         settings=wandb.Settings(start_method=\"thread\"),\n",
        "#         project=\"Kaggle-Intel-Image\",\n",
        "#         name='dropout_tuned',\n",
        "#         group='CNN',\n",
        "#         config={\"epochs\":n_epochs, \"batch\": BATCH_SIZE, \"transforms\": train_transforms},\n",
        "#         sync_tensorboard=False,\n",
        "#         resume=\"allow\",\n",
        "#     ):\n",
        "#         train(feat_model, criterion, optimizer, n_epochs, BATCH_SIZE, train_transforms, lr_scheduler=lr_sch, saving_model_path='feat_model', wb=True)"
      ],
      "metadata": {
        "id": "2v5eJ9NfGopX",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 48
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Зменшення feature map з 75 до 16 не лише зменшило продуктивність вдвічі, а і зробила будь-які паттерни створені CNN занадто малими, щоб бути корисними для моделі\n",
        "\n",
        "- Validation accuracy нижчий за Test, що може свідчити про певну користь від аугментацій, коли модель не страждає від ускладнень зображення, пов'язаних з ними.\n",
        "\n"
      ],
      "metadata": {
        "id": "sJzoJunONokH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BetterModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 32 * 3 -> 16 -> 16\n",
        "        self.bn11 = nn.BatchNorm2d(3)\n",
        "        self.conv11 = nn.Conv2d(3, 75, 3, padding=\"same\") #3/16\n",
        "        self.bn12 = nn.BatchNorm2d(75) # 16\n",
        "        self.conv12 = nn.Conv2d(75, 75, 3, padding=\"same\") #16/16\n",
        "        self.bn21 = nn.BatchNorm2d(75) # 16\n",
        "        self.conv21 = nn.Conv2d(75, 450, 3, padding=\"same\") #16/32\n",
        "        self.bn22 = nn.BatchNorm2d(450) # 32\n",
        "        self.conv22 = nn.Conv2d(450, 450, 3, padding=\"same\") #32/32 -> 150 * 37 * 37\n",
        "        # self.bn31 = nn.BatchNorm2d(75) # 16\n",
        "        self.conv31 = nn.Conv2d(450, 1350, 3, padding=\"same\") #16/32 -> 450*9*9\n",
        "        # self.bn32 = nn.BatchNorm2d(150) # 32\n",
        "        self.conv32 = nn.Conv2d(1350, 1350, 3, padding=\"same\") #32/32 -> 1350*2*2\n",
        "        self.fc1 = nn.Linear(5400, 512) # 32 * 64\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, len(classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv11(self.bn11(x)))\n",
        "        x=  F.relu(self.conv12(self.bn12(x)))\n",
        "        x = F.max_pool2d(x, 4)\n",
        "        # print(\"\\n\", x.shape)\n",
        "        x = F.relu(self.conv21(self.bn21(x)))\n",
        "        x = F.relu(self.conv22(self.bn22(x)))\n",
        "        x = F.max_pool2d(x, 4)\n",
        "        # print(\"\\n\", x.shape)\n",
        "        x = F.relu(self.conv31(x))\n",
        "        x = F.relu(self.conv32(x))\n",
        "        x = F.max_pool2d(x, 4)\n",
        "        # print(\"\\n\", x.shape)\n",
        "        x = x.view(-1, 5400)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # x = F.dropout(F.relu(self.fc2(x)), 0.2)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "40BPh2EDRMIx",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 76
    },
    {
      "cell_type": "markdown",
      "source": [
        "17% accuracy, 0.065% f1 після 5 епох - Не будемо далі тренувати"
      ],
      "metadata": {
        "id": "uDHzRbJD23DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 200"
      ],
      "metadata": {
        "trusted": true,
        "id": "CinZY2qjYvit"
      },
      "outputs": [],
      "execution_count": 63
    },
    {
      "cell_type": "code",
      "source": [
        "model = BetterModel().to(device)"
      ],
      "metadata": {
        "id": "09_kK5dY82aF"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnRoViZ483SE",
        "outputId": "e7deebf8-dd88-485a-d529-cf3427f3e321"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BetterModel(\n",
              "  (bn11): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv11): Conv2d(3, 75, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (bn12): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv12): Conv2d(75, 75, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (bn21): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv21): Conv2d(75, 450, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (bn22): BatchNorm2d(450, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv22): Conv2d(450, 450, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (conv31): Conv2d(450, 1350, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (conv32): Conv2d(1350, 1350, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (fc1): Linear(in_features=5400, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with wandb.init(\n",
        "          settings=wandb.Settings(start_method=\"thread\"),\n",
        "          project=\"Kaggle-Intel-Image\",\n",
        "          name='2nd_batch_norm',\n",
        "          group='CNN',\n",
        "          config={\"epochs\":n_epochs, \"batch\": BATCH_SIZE, \"transforms\": train_transforms},\n",
        "          sync_tensorboard=False,\n",
        "          resume=\"allow\",\n",
        "      ):\n",
        "          train(model, criterion, optimizer, n_epochs, BATCH_SIZE, train_transforms, lr_scheduler=lr_sch, saving_model_path='2nd_batch_norm', wb=True)"
      ],
      "metadata": {
        "id": "yUWRjkmiSS3Q",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "980aa43f-1f77-4bdf-ba74-569c1e51b975"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250317_130053-dnjfezq3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/paradoxv15/Kaggle-Intel-Image/runs/dnjfezq3' target=\"_blank\">2nd_batch_norm</a></strong> to <a href='https://wandb.ai/paradoxv15/Kaggle-Intel-Image' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/paradoxv15/Kaggle-Intel-Image' target=\"_blank\">https://wandb.ai/paradoxv15/Kaggle-Intel-Image</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/paradoxv15/Kaggle-Intel-Image/runs/dnjfezq3' target=\"_blank\">https://wandb.ai/paradoxv15/Kaggle-Intel-Image/runs/dnjfezq3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in function len>\n",
            "\n",
            "Fitting nn model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:   1, loss: 1.79200, valid loss: 1.79040, valid accuracy: 17.890, valid f1: 0.054%\n",
            "Validation loss decreased (inf --> 1.790397). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/200 [02:54<9:39:56, 174.86s/it]/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:   2, loss: 1.79202, valid loss: 1.79020, valid accuracy: 17.926, valid f1: 0.055%\n",
            "Validation loss decreased (1.790397 --> 1.790198). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 2/200 [05:27<8:54:06, 161.85s/it]/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "  2%|▏         | 3/200 [08:00<8:38:43, 157.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:   3, loss: 1.79197, valid loss: 1.79041, valid accuracy: 17.890, valid f1: 0.054%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\r  2%|▏         | 4/200 [10:32<8:27:55, 155.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:   4, loss: 1.79195, valid loss: 1.79034, valid accuracy: 17.890, valid f1: 0.054%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:   5, loss: 1.79197, valid loss: 1.79018, valid accuracy: 17.890, valid f1: 0.054%\n",
            "Validation loss decreased (1.790198 --> 1.790176). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▎         | 5/200 [13:04<8:21:30, 154.31s/it]/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "  3%|▎         | 6/200 [15:37<8:16:33, 153.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:   6, loss: 1.79200, valid loss: 1.79035, valid accuracy: 17.890, valid f1: 0.054%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\r  4%|▎         | 7/200 [18:09<8:13:11, 153.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:   7, loss: 1.79204, valid loss: 1.79038, valid accuracy: 17.926, valid f1: 0.055%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\r  4%|▍         | 8/200 [20:41<8:09:03, 152.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:   8, loss: 1.79200, valid loss: 1.79038, valid accuracy: 17.890, valid f1: 0.054%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\r  4%|▍         | 9/200 [23:13<8:05:16, 152.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:   9, loss: 1.79200, valid loss: 1.79035, valid accuracy: 17.890, valid f1: 0.054%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\r  5%|▌         | 10/200 [25:44<8:01:35, 152.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:  10, loss: 1.79199, valid loss: 1.79030, valid accuracy: 17.926, valid f1: 0.055%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\r  6%|▌         | 11/200 [28:19<8:02:19, 153.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:  11, loss: 1.79203, valid loss: 1.79038, valid accuracy: 17.890, valid f1: 0.054%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\r  6%|▌         | 12/200 [30:58<8:04:59, 154.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:  12, loss: 1.79200, valid loss: 1.79041, valid accuracy: 17.926, valid f1: 0.055%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 12/200 [31:19<8:10:39, 156.59s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-79-f9a6852f99f1>\", line 10, in <cell line: 0>\n",
            "    train(model, criterion, optimizer, n_epochs, BATCH_SIZE, train_transforms, lr_scheduler=lr_sch, saving_model_path='2nd_batch_norm', wb=True)\n",
            "  File \"<ipython-input-67-957083824c63>\", line 23, in train\n",
            "    train_losses, final_step = fit(n_epochs, model, criterion, optimizer, loader_train, loader_eval, lr_scheduler)\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-66-9408a1b1086c>\", line 13, in fit\n",
            "    for X, y  in train_loader:\n",
            "  File \"<ipython-input-18-51c394062ba0>\", line 10, in __iter__\n",
            "    for batch in iter(self.loader):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 764, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n",
            "    data = self.dataset.__getitems__(possibly_batched_index)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n",
            "    return [self.dataset[self.indices[idx]] for idx in indices]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n",
            "    return [self.dataset[self.indices[idx]] for idx in indices]\n",
            "            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n",
            "    sample = self.loader(path)\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n",
            "    return pil_loader(path)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\", line 262, in pil_loader\n",
            "    with open(path, \"rb\") as f:\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">2nd_batch_norm</strong> at: <a href='https://wandb.ai/paradoxv15/Kaggle-Intel-Image/runs/dnjfezq3' target=\"_blank\">https://wandb.ai/paradoxv15/Kaggle-Intel-Image/runs/dnjfezq3</a><br> View project at: <a href='https://wandb.ai/paradoxv15/Kaggle-Intel-Image' target=\"_blank\">https://wandb.ai/paradoxv15/Kaggle-Intel-Image</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250317_130053-dnjfezq3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-f9a6852f99f1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"allow\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       ):\n\u001b[0;32m---> 10\u001b[0;31m           \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_sch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2nd_batch_norm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-957083824c63>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, n_epochs, batch_size, train_transforms, lr_scheduler, saving_model_path, wb)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'length array:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Fit time: {time.time() - start_time} s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-9408a1b1086c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, optimizer, train_loader, valid_loader, lr_scheduler, patience)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m           \u001b[0;31m# print(X.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-51c394062ba0>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 79
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Результати за посиланням на [wandb](https://wandb.ai/paradoxv15/Kaggle-Intel-Image/reports/Model-tuning-for-Intel-Image-classifier--VmlldzoxMTgxOTgwMA)\n",
        "\n"
      ],
      "metadata": {
        "id": "CSPxkWwwx6AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_examples(model, loss_func, X, y):\n",
        "    output = model(X)\n",
        "    pred = torch.argmax(output, dim=1)\n",
        "    correct = pred == y.view(*pred.shape)\n",
        "\n",
        "    return X, pred, y.view(*pred.shape)"
      ],
      "metadata": {
        "id": "GiCC4FAsx6Aj",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_predicted_examples(image_array, grid_x, grid_y, title):\n",
        "    fig = plt.figure(figsize=(grid_x,grid_y))\n",
        "    fig.suptitle(title, fontsize=20)\n",
        "\n",
        "    for i in range(1,grid_y*grid_x+1):\n",
        "        index = random.randint(0, len(image_array)-1)\n",
        "        image = image_array[index]\n",
        "        plt.axis('off')\n",
        "        plt.subplot(grid_y,grid_x,i)\n",
        "        plt.imshow(np.transpose(image.numpy(), (1, 2, 0)))"
      ],
      "metadata": {
        "id": "GA133B5Sx6Aj",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}